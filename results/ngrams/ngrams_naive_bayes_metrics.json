{
  "accuracy": 0.7826014637391883,
  "macro_precision": 0.783507872293562,
  "macro_recall": 0.7881343713753285,
  "macro_f1": 0.7784367326217873,
  "per_class_metrics": {
    "Business": {
      "precision": 0.7381799694966955,
      "recall": 0.7798066595059077,
      "f1-score": 0.7584225646382867,
      "f1": 0.7584225646382867,
      "support": 1862
    },
    "Entertainment": {
      "precision": 0.7688113413304253,
      "recall": 0.9227748691099477,
      "f1-score": 0.8387864366448543,
      "f1": 0.8387864366448543,
      "support": 2292
    },
    "Health": {
      "precision": 0.6084905660377359,
      "recall": 0.7811418685121108,
      "f1-score": 0.6840909090909091,
      "f1": 0.6840909090909091,
      "support": 1156
    },
    "Politics": {
      "precision": 0.8238372093023256,
      "recall": 0.8567110036275696,
      "f1-score": 0.8399525785417902,
      "f1": 0.8399525785417902,
      "support": 1654
    },
    "Science": {
      "precision": 0.7957992998833139,
      "recall": 0.7118997912317327,
      "f1-score": 0.7515151515151515,
      "f1": 0.7515151515151515,
      "support": 958
    },
    "Sports": {
      "precision": 0.9265330904675168,
      "recall": 0.9304878048780488,
      "f1-score": 0.9285062366899909,
      "f1": 0.9285062366899909,
      "support": 1640
    },
    "Technology": {
      "precision": 0.8229036295369212,
      "recall": 0.5341186027619821,
      "f1-score": 0.6477832512315271,
      "f1": 0.6477832512315271,
      "support": 2462
    }
  },
  "num_samples": 12024,
  "training_time_seconds": 41.49839687347412
}